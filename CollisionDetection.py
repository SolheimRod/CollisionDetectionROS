#! /usr/bin/env python3
import pyrealsense2 as rs
import numpy, ros_numpy, rospy, sensor_msgs, tf, math, std_msgs, actionlib_msgs
import cv_bridge
import json
from visualization_msgs.msg import Marker, MarkerArray

#█▀▀ █▀█ █▄░█ █▀▀ █ █▀▀
#█▄▄ █▄█ █░▀█ █▀░ █ █▄█

#can be found in the "realsense-viewer" application or on the box for the camera. 
#This is the camera used for collision detection, only one camera is used for detection.
#This camera should be placed directly above the base of the robot, facing down.
main_camera_serial = "031422250715"

#First bootup of the system, set to false after initial image is taken
#Should be True / False   (Case-Sensitive)
#bool
startup = False

#amount of "spheres" to check for collision with pointcloud data
#a value of 4 is a good balance between safety and performance
#int
buffer_density = 6

#radius of sphere around robot to detect collision, in meters. 
#Must be smaller than the distance between the robot and floor.
#float
stop_radius = 0.7

#radius for self filtering, must be smaller than stop_radius.
#float
filter_radius = 0.65

#used to filter away pointcloud generated by objects with known transfer-frame-data,...
#...such as other robots that should not trigger collision with the robot using this code for collision detection.
#only used if an "ojbect" list with x,y,z points is crated for known objects.
#float
object_filter_radius = 0.3

#Define TF data (transfer-frame) between camera and the "world".
#(distance between camera and the ground-floor):
#float, meters.
camera_height = 2.73
#Camera x,y distance between origin of world-frame and camera-frame.
#Optimally should be set to 0, 0... But can be used for fine adjustments when aligning pointcloud data with robot transfer-frame. 
#float(s), meters.
camera_x = 0.07
camera_y = -0.04

#camera rotation in radians relative to the world, can be set to 0, 0, 0... or used to align the robot with the pointcloud data
#floats, radians.
camera_rot = (0, math.pi, 0)

#Same position configuration as above for the camera, only this time for the robot.
#The "origin" of the world frame is straight below the robot, meaning always 0, 0. While the height may differ.
#This is the robots (the base of the robot) height above the floor:
#float, meters.
robot_height = 0.74

#floats, radians.
robot_rot = (0, 0, -0.7854)

#█▀█ █▀█ ▀█▀ █ █▀█ █▄░█ ▄▀█ █░░   █▀▀ █▀█ █▄░█ █▀▀ █ █▀▀
#█▄█ █▀▀ ░█░ █ █▄█ █░▀█ █▀█ █▄▄   █▄▄ █▄█ █░▀█ █▀░ █ █▄█

#Used to define the serial codes of the other cameras, several cameras can be used for collision avoidance.
#Should look like: ["serial1", ... , "serialn"].
#It is advised to setup the main camera first before attempting to setup any secondary cameras.
#it is also advised to setup one secondary camera at a time.

#MAX 3  additional secondary cameras are supported
#Secondary camera used in the lab serial: "031422250497"
#Example: secondary_cameras = ["031422250497"]
secondary_cameras = []

#define tf data of each secondary camera.
#Should look like: [(x1, y1, z1, r1, p1, y1), ... , (xn, yn, zn, rn, pn, yn)]
#Example: secondary_cameras_tf = [(0.4, 2.85, 0.74, -math.pi/2, 0, math.pi)]
#floats, radians
secondary_cameras_tf = []

#how many times to "project" the pointcloud in z-direction (down)
#this will fill blindzones with voxel-occupany-grid
#This value should be the lowest possible for performance
#but high enough to fill all blindzones, use Rviz to see the results
#int
projection_amount = 10

#decimation filter of SECONDARY cameras only, higher number = less points (more compressed data) = better performance.
#min value: 1, max: 8.
#int
decimval = 3

#▄▄ ▄▄ ▄▄   █▀▀ █▀█ █▄░█ █▀▀ █ █▀▀   █▀▀ █▄░█ █▀▄   ▄▄ ▄▄ ▄▄
#░░ ░░ ░░   █▄▄ █▄█ █░▀█ █▀░ █ █▄█   ██▄ █░▀█ █▄▀   ░░ ░░ ░░

#Camera filters, tuned for maximum precision, should probably not be changed >:(
dispfilter = rs.disparity_transform(True)
tempfilter = rs.temporal_filter(0.4, 50, 1)
decimfilter = rs.decimation_filter(3)
undispfilter = rs.disparity_transform(False)

with open("cameracfg.json") as jsonFile:
  jsonObj = json.load(jsonFile)
  jsonFile.close()

json_string= str(jsonObj).replace("'", '\"')

pipe = rs.pipeline()
cfg = rs.config()
pc = rs.pointcloud()
cfg.enable_device(main_camera_serial)
cfg.enable_stream(rs.stream.depth, 640, 480, rs.format.z16, 30)
cfg.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 30)
profile = pipe.start(cfg)

dev = profile.get_device()
advmode = rs.rs400_advanced_mode(dev)
advmode.load_json(json_string)

print("Main Camera Connected...")

pipes = []
cfgs = []
pcs = []
profiles = []
devs = []
advmodes = []

secondarydecimfilter = []
secondarydispfilter = []
secondarytempfilter = []
secondaryundispfilter = []

expsets = []

#There has to be individual filters for each camera:
if secondary_cameras != []:
  for each in range(len(secondary_cameras)):
    pipes.append(rs.pipeline())
    cfgs.append(rs.config())
    pcs.append(rs.pointcloud())
    cfgs[each].enable_device(secondary_cameras[each])
    cfgs[each].enable_stream(rs.stream.depth, 640, 480, rs.format.z16, 30)
    profiles.append(pipes[each].start(cfgs[each]))
    secondarydecimfilter.append(rs.decimation_filter(decimval))
    secondarydispfilter.append(rs.disparity_transform(True))
    secondarytempfilter.append(rs.temporal_filter(0.4, 50, 1))
    secondarydecimfilter.append(rs.decimation_filter(3))
    secondaryundispfilter.append(rs.disparity_transform(False))
    
    devs.append(profiles[each].get_device())
    advmodes.append(rs.rs400_advanced_mode(devs[each]))
    advmodes[each].load_json(json_string)
    
    expsets.append(profiles[each].get_device().query_sensors()[1])
    expsets[each].set_option(rs.option.enable_auto_exposure, False)
    expsets[each].set_option(rs.option.enable_auto_white_balance, False)
    expsets[each].set_option(rs.option.auto_exposure_priority, False)

print(str(len(pipes)) + " Secondary Cameras Connected...")

align = rs.align(rs.stream.color)

frame = pipe.wait_for_frames()
aligned_frame = align.process(frame)
processed_frame = aligned_frame.get_depth_frame()
processed_frame = dispfilter.process(processed_frame)
processed_frame = tempfilter.process(processed_frame)
processed_frame = undispfilter.process(processed_frame)

toggle_update = 1
def running_callback(data):
    global init_var1, frame, toggle_update
    if len(data.status_list) > 0:
        current_status = data.status_list[len(data.status_list) - 1].status
        if current_status == 3:
          init_var1 = 1
          if toggle_update == 1:
            toggle_update = 0
        else: 
          toggle_update = 1

expset = profile.get_device().query_sensors()[1]
expset.set_option(rs.option.enable_auto_exposure, False)
expset.set_option(rs.option.enable_auto_white_balance, False)
expset.set_option(rs.option.auto_exposure_priority, False)
expset.set_option(rs.option.saturation, 100)
expset.set_option(rs.option.exposure, 120)

rospy.init_node("MainNode")
listener = tf.TransformListener()
br = tf.TransformBroadcaster()

cam = rospy.Publisher("img", sensor_msgs.msg.Image, queue_size=1)
img_pub = rospy.Publisher("filtered_img", sensor_msgs.msg.Image, queue_size=1)
msg_pub = rospy.Publisher("collision_detection", std_msgs.msg.String, queue_size=1)
pc_pub = rospy.Publisher("collision_detection_raw_point_cloud", sensor_msgs.msg.PointCloud2, queue_size=1)
detected_pub = rospy.Publisher("camera/depth/color/detectedpoints", sensor_msgs.msg.PointCloud2, queue_size=1)
processed_pub = rospy.Publisher("camera/depth/color/points", sensor_msgs.msg.PointCloud2, queue_size=1)
rospy.Subscriber("/move_group/status", actionlib_msgs.msg.GoalStatusArray, running_callback, queue_size=1)  

secondary_pubs = []

if secondary_cameras != []:
  for each in range(len(secondary_cameras)):
    secondary_pubs.append(rospy.Publisher("secondary_camera_"+str(each), sensor_msgs.msg.PointCloud2, queue_size=1))

marker_pub = rospy.Publisher("visualization_marker_array", MarkerArray, queue_size = 1)
marker = Marker()

init_var1 = 1
init_varpc = 1

camera_tf = (camera_x, camera_y, camera_height)

if len(secondary_cameras) != len(secondary_cameras_tf):
  print("------------------------------------------------------------------")
  print("secondary_cameras and secondary_cameras_tf must be equal in length")
  print("------------------------------------------------------------------")
  exit()

if filter_radius > stop_radius:
  print("-------------------------------------------------")
  print("filter_radius can not be bigger than stop_radius!")
  print("-------------------------------------------------")
  exit()

if buffer_density <= 3:
  print("-------------------------------------")
  print("buffer_density must be higher than 3!")
  print("-------------------------------------") 
  exit()

if (buffer_density % 2) != 0:
  buffer_density = buffer_density + 1

def tfdata(listener):
  try:
    listener.waitForTransform('/camera_link', '/base_link', rospy.Time(0), rospy.Duration(0.1))
    (trans, rot) = listener.lookupTransform('/camera_link', '/base_link', rospy.Time(0))
    (transferone, rot) = listener.lookupTransform('/camera_link', '/wrist_2_link', rospy.Time(0))
    (transfertwo, rot) = listener.lookupTransform('/camera_link', '/forearm_link', rospy.Time(0))
    return trans, transferone, transfertwo
  except:
    pass

def filter(color_frame):
  color_array = numpy.array(color_frame)
  global rgb_init_array, init_var1, init_varpc
  if init_var1 == 1:
    rgb_init_array = color_array
    init_var1 = 0
    init_varpc = 1
  close_array = numpy.isclose(color_array, rgb_init_array, atol=90)
  close_array_compressed = numpy.invert(close_array[:,:,0] * close_array[:,:,1] * close_array[:,:,2]).reshape(480,640) 
  return close_array_compressed

def convert(diffarray, processed_frame):
  global init_varpc, startup, init_posmat, linepoints, filter_radius, blindpos, dyn_posmat, secondframe, firstframe
  cal = pc.calculate(processed_frame)
  PCdata = numpy.array(cal.get_vertices())
  PCdata.dtype = [('x', '<f4'),('y', '<f4'),('z', '<f4')]
  
  if startup == True:
    init_posmat = numpy.stack([PCdata['x'], PCdata['y'], PCdata['z']], axis=-1)
    numpy.save(open("data.npy", "wb"), init_posmat)
    print("-----------------------------------------------------")
    print("Static Environment Saved, Please set startup to False")
    print("-----------------------------------------------------")
    exit()
  
  if init_varpc == 1:
    blindpos = linepoints
    dyn_posmat = PCdata
    init_varpc = 0

  PCdata[((abs(PCdata['x'] - init_posmat[:,0]) + abs(PCdata['y'] - init_posmat[:,1]) + abs(PCdata['z'] - init_posmat[:,2])) < 0.5)] = 0
  PCdata[((abs(PCdata['x'] - dyn_posmat['x']) + abs(PCdata['y'] - dyn_posmat['y']) + abs(PCdata['z'] - dyn_posmat['z'])) < 0.4)] = 0

  posmat = PCdata
  
  posmatx = numpy.multiply(diffarray.flatten(), posmat['x'])
  posmaty = numpy.multiply(diffarray.flatten(), posmat['y'])
  posmatz = numpy.multiply(diffarray.flatten(), posmat['z'])
  posmat = numpy.stack([posmatx, posmaty, posmatz], axis=-1)
  return posmat

stop_radius = stop_radius - 0.1
filter_radius = filter_radius - 0.1

def pcpub(data, floor_distance, p):
  global linepoints, filter_radius
  data = decimfilter.process(data)
  cal = pc.calculate(data)
  PCdata = numpy.asarray(cal.get_vertices())
  PCdata.dtype = [('x', '<f4'),('y', '<f4'),('z', '<f4')]
  pcrawviz = ros_numpy.point_cloud2.array_to_pointcloud2(cloud_arr=PCdata, stamp=rospy.Time.now(), frame_id="camera_link")
  pc_pub.publish(pcrawviz)
  PCdata = PCdata[PCdata['z'] > 1]

  selffiltered_pc = PCdata

  for each in range(len(linepoints)):
    selffiltered_pc = selffiltered_pc[((selffiltered_pc['x'] - linepoints[each,0])**2 + (selffiltered_pc['y'] - linepoints[each,1])**2 + (selffiltered_pc['z'] - (linepoints[each,2]))**2) > 0.3**2]
  selffiltered_pc = numpy.stack((selffiltered_pc['x'], selffiltered_pc['y'], selffiltered_pc['z']), axis=-1)

  projected_pc = numpy.stack((PCdata['x'], PCdata['y'], PCdata['z']), axis=-1)
  
  for each in range(p):
    appendable = numpy.stack((selffiltered_pc[:,0], selffiltered_pc[:,1], (selffiltered_pc[:,2] + each/7 + 0.1)), axis=-1)
    projected_pc = numpy.append(projected_pc, appendable, axis=0)
  
  projected_pc.dtype = [('x', '<f4'),('y', '<f4'),('z', '<f4')]
  projected_pc = projected_pc[projected_pc['z'] < floor_distance+0.1]

  projected_pc = ros_numpy.point_cloud2.array_to_pointcloud2(cloud_arr=projected_pc, stamp=rospy.Time.now(), frame_id="camera_link")
  processed_pub.publish(projected_pc)

def markers():
  global marker_array_msg
  marker_array_msg = MarkerArray()
  for each in range(len(linepoints[:,0])):
      marker = Marker()
      marker.header.frame_id = "markers"
      marker.header.stamp = rospy.Time.now()
      marker.id = each
      marker.type = 2
      marker.pose.position.x = linepoints[each,0]
      marker.pose.position.y = linepoints[each,1]
      marker.pose.position.z = linepoints[each,2]
      marker.pose.orientation.x = 0.0
      marker.pose.orientation.y = 0.0
      marker.pose.orientation.z = 0.0
      marker.pose.orientation.w = 1.0
      marker.color.r = 1.0
      marker.color.g = 0.0
      marker.color.b = 0.0
      marker.color.a = 0.3
      marker.scale.x = stop_radius*2
      marker.scale.y = stop_radius*2
      marker.scale.z = stop_radius*2
      marker_array_msg.markers.append(marker) 

if startup != True:
  init_posmat = numpy.load(open("data.npy", "rb"))

basetfname = "secondary_camera_"
print("Broadcasting TF data before starting...")
rospy.sleep(5)
print("Starting.")

quaternionrobot = tf.transformations.quaternion_from_euler(robot_rot[0], robot_rot[1], robot_rot[2])
quaternion = tf.transformations.quaternion_from_euler(camera_rot[0], camera_rot[1], camera_rot[2])

tfnow = rospy.Time.now()
br.sendTransform((0,0,robot_height), quaternionrobot, tfnow, "base_link", "world")
br.sendTransform((camera_tf), quaternion, tfnow, "camera_link", "world")
br.sendTransform((0,0,0), (0,0,0,1), tfnow, "markers", "camera_link")

while not rospy.is_shutdown():
  tfnow = rospy.Time.now()
  br.sendTransform((0,0,robot_height), quaternionrobot, tfnow, "base_link", "world")
  br.sendTransform((camera_tf), quaternion, tfnow, "camera_link", "world")
  if secondary_cameras != []:
    for each in range(len(secondary_cameras)):
      secondary_quarternion = tf.transformations.quaternion_from_euler(secondary_cameras_tf[each][3], secondary_cameras_tf[each][4], secondary_cameras_tf[each][5])
      tfname = basetfname + str(each)
      secondary_frame = pipes[each].wait_for_frames()
      
      secondary_processed_frame = secondary_frame.get_depth_frame()
      secondary_processed_frame = secondarydispfilter[each].process(secondary_processed_frame)
      secondary_processed_frame = secondarytempfilter[each].process(secondary_processed_frame)
      secondary_processed_frame = secondaryundispfilter[each].process(secondary_processed_frame)
      secondary_processed_frame = secondarydecimfilter[each].process(secondary_processed_frame)
      
      secondary_cal = pcs[each].calculate(secondary_processed_frame)
      secondary_pcdata = numpy.asarray(secondary_cal.get_vertices())
      secondary_pcdata.dtype = [('x', '<f4'),('y', '<f4'),('z', '<f4')]
      secondary_pcpubdata = ros_numpy.point_cloud2.array_to_pointcloud2(cloud_arr=secondary_pcdata, stamp=rospy.Time.now(), frame_id=tfname)
      br.sendTransform((secondary_cameras_tf[each][0], secondary_cameras_tf[each][1], secondary_cameras_tf[each][2],), secondary_quarternion, rospy.Time.now(), tfname, "world")
      secondary_pubs[each].publish(secondary_pcpubdata)

  trans, realtransone, realtranstwo = tfdata(listener)
  frame = pipe.wait_for_frames()
  aligned_frame = align.process(frame)
  processed_frame = aligned_frame.get_depth_frame()
  processed_frame = dispfilter.process(processed_frame)
  processed_frame = tempfilter.process(processed_frame)
  processed_frame = undispfilter.process(processed_frame)
  
  color_frame = frame.get_color_frame().get_data()
  white_array = numpy.zeros([480,640],dtype=numpy.uint8)
  white_array.fill(255)
  diff_array = numpy.array(filter(color_frame))
  
  cammie = numpy.array(color_frame)
  cam.publish(cv_bridge.CvBridge().cv2_to_imgmsg(cammie, encoding="passthrough"))

  img_array = numpy.multiply(diff_array, white_array)

  if trans is not None:
    
    hpointsone = numpy.linspace(trans[2], realtranstwo[2], buffer_density/2)
    hpointstwo = numpy.linspace(realtranstwo[2], realtransone[2], buffer_density/2)
    xpointsone = numpy.linspace(trans[0], realtranstwo[0], buffer_density/2)
    xpointstwo = numpy.linspace(realtranstwo[0], realtransone[0], buffer_density/2)
    ypointsone = numpy.linspace(trans[1], realtranstwo[1], buffer_density/2)
    ypointstwo = numpy.linspace(realtranstwo[1], realtransone[1], buffer_density/2)
    hpoints = numpy.concatenate((hpointsone, hpointstwo))
    xpoints = numpy.concatenate((xpointsone, xpointstwo))
    ypoints = numpy.concatenate((ypointsone, ypointstwo))
      
    linepoints = numpy.stack([xpoints, ypoints, hpoints], axis=-1)
    pcpub(processed_frame, camera_height, projection_amount)
    
    posmat = convert(diff_array, processed_frame)
    
    for each in range(len(linepoints)):
      posmat = posmat[((posmat[:,0] - linepoints[each,0])**2 + (posmat[:,1] - linepoints[each,1])**2 + (posmat[:,2] - (linepoints[each,2]+0.3))**2) > filter_radius**2]
    
    posmat = posmat[((posmat[:,0] - 0)**2 + (posmat[:,1] - 0)**2 + (posmat[:,2] - 0.1)**2) > (filter_radius+((filter_radius+stop_radius)/2) + 0.2)**2]
  
    if 'objects' in globals():
      if objects != []:
       for each in range(len(objects)):
          posmat = posmat[((posmat[:,0] - objects[each][0])**2 + (posmat[:,1] - objects[each][1])**2 + (posmat[:,2] - (objects[each][2]))**2) > object_filter_radius**2]
      
    for each in range(len(linepoints)):
      if (stop_radius**2 > ((posmat[:,0] - linepoints[each,0])**2 + (posmat[:,1] - linepoints[each,1])**2 + (posmat[:,2] - linepoints[each,2])**2)).any():
        msg_pub.publish("stop")
        print ("Stop!", tfnow)
        init_var1 = 1
      else:
        msg_pub.publish("clear")
  else:
    msg_pub.publish("stop")
    print("No TF Data!", tfnow)
  
  posmat.dtype = [('x', '<f4'),('y', '<f4'),('z', '<f4')] 
  pcviz = ros_numpy.point_cloud2.array_to_pointcloud2(cloud_arr=posmat, stamp=rospy.Time.now(), frame_id="camera_link")
  detected_pub.publish(pcviz)
  
  markers()
  br.sendTransform((0,0,0), (0,0,0,1), rospy.Time.now(), "markers", "camera_link")
  marker_pub.publish(marker_array_msg)
  
  img_pub.publish(cv_bridge.CvBridge().cv2_to_imgmsg(img_array, encoding="passthrough"))

pipe.stop()

